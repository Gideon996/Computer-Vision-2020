{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST ATTEMPT TO CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this first file is to implement a Convolutional Neural Network able to classify the images in the different classes with an accuracy of at least 30%.\n",
    "\n",
    "Initially, the data must be read and pre-processed. In general it is necessary to convert the images to grayscale and resize the various images to 64x64 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libreries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of list of libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dataSetUtility.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "import keras as ks\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import import_ipynb\n",
    "import dataSetUtility as dsu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we construct and prepare the data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class number:  15\n",
      "Class names:  ['Bedroom', 'Coast', 'Forest', 'Highway', 'Industrial', 'InsideCity', 'Kitchen', 'LivingRoom', 'Mountain', 'Office', 'OpenCountry', 'Store', 'Street', 'Suburb', 'TallBuilding']\n"
     ]
    }
   ],
   "source": [
    "#path to the directories\n",
    "pathTrain=r'C:\\Users\\adria\\Desktop\\CVPR\\ImageSet\\train'\n",
    "pathTest=r'C:\\Users\\adria\\Desktop\\CVPR\\ImageSet\\test'\n",
    "\n",
    "labels = [os.path.basename(i) for i in glob.glob(pathTrain + '/*', recursive=True)]\n",
    "numberOfClasses = len(labels)\n",
    "print(\"Class number: \", numberOfClasses)\n",
    "print(\"Class names: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train x:  1500\n",
      "len test x:  2985\n",
      "len train y:  1500\n",
      "len test y:  2985\n"
     ]
    }
   ],
   "source": [
    "xTrainRaw, yTrainRaw = dsu.loadImages(pathTrain, labels)\n",
    "xTest, yTest = dsu.loadImages(pathTest, labels)\n",
    "\n",
    "print('len train x: ',len(xTrainRaw))\n",
    "print('len test x: ',len(xTest))\n",
    "print('len train y: ',len(yTrainRaw))\n",
    "print('len test y: ',len(yTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we resize the images in a correct dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainRaw = dsu.reshape(xTrainRaw, 64,64,1)\n",
    "xTest = dsu.reshape(xTest,64,64,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the labels into a set of numbers to input into the neural net work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainCategorical = to_categorical(yTrainRaw)\n",
    "yTestCategorical = to_categorical(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training set into train set (85%) and validation set (15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Train Set:  1275\n",
      "len Validation Set:  225\n",
      "len Test Set:  2985\n"
     ]
    }
   ],
   "source": [
    "xTrain, xValidation, yTrain, yValidation = train_test_split(xTrainRaw, yTrainCategorical, train_size=0.85, random_state=275)\n",
    "print('len Train Set: ',len(xTrain))\n",
    "print('len Validation Set: ',len(xValidation))\n",
    "print('len Test Set: ',len(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To buil the model we need to create the architecture and add the layer to extract features from the input images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = ks.optimizers.SGD(momentum=0.9,nesterov=True)\n",
    "norm = ks.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "baseModel = Sequential([\n",
    "    #first convolutional layer\n",
    "    Conv2D(8, 3,strides=1, padding='valid',input_shape=(64,64,1)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=2,strides=2),\n",
    "    \n",
    "    #second convolutional layer\n",
    "    Conv2D(16, 3,strides=1, padding='valid',input_shape=(64,64,1)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=2,strides=2),\n",
    "    \n",
    "    #third convolutional layer\n",
    "    Conv2D(32, 3,strides=1, padding='valid',input_shape=(64,64,1)),\n",
    "    Activation('relu'),\n",
    "    Flatten(),\n",
    "    Dense(numberOfClasses, activation='relu',kernel_initializer=norm, bias_initializer='zeros'),\n",
    "    Dense(numberOfClasses, activation='softmax')\n",
    "])\n",
    "\n",
    "baseModel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRANING AND TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our CNN and visualizze the training results. We also implement the earlystopping in order to stop the training before the end of epochs if it is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 3s 53ms/step - loss: 2.7090 - accuracy: 0.0637 - val_loss: 2.7100 - val_accuracy: 0.0667\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 2.7079 - accuracy: 0.0702 - val_loss: 2.7094 - val_accuracy: 0.0711\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 1s 23ms/step - loss: 2.7067 - accuracy: 0.0841 - val_loss: 2.7097 - val_accuracy: 0.0533\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 2.7039 - accuracy: 0.0756 - val_loss: 2.7084 - val_accuracy: 0.0667\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 2.7000 - accuracy: 0.0834 - val_loss: 2.7036 - val_accuracy: 0.0756\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 2.6883 - accuracy: 0.1146 - val_loss: 2.7234 - val_accuracy: 0.0533\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 2.6566 - accuracy: 0.1044 - val_loss: 2.6396 - val_accuracy: 0.1022\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 2.5738 - accuracy: 0.1594 - val_loss: 2.5409 - val_accuracy: 0.1733\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 1s 27ms/step - loss: 2.4614 - accuracy: 0.1900 - val_loss: 2.5248 - val_accuracy: 0.1867\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 1s 26ms/step - loss: 2.3565 - accuracy: 0.2436 - val_loss: 2.5201 - val_accuracy: 0.1733\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 2.2444 - accuracy: 0.2656 - val_loss: 2.3377 - val_accuracy: 0.2356\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 2.1022 - accuracy: 0.3177 - val_loss: 2.3412 - val_accuracy: 0.2622\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 1s 27ms/step - loss: 1.8830 - accuracy: 0.3643 - val_loss: 2.3382 - val_accuracy: 0.2311\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 1s 26ms/step - loss: 1.7266 - accuracy: 0.4346 - val_loss: 2.1983 - val_accuracy: 0.2844\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 1s 25ms/step - loss: 1.5186 - accuracy: 0.4927 - val_loss: 2.0157 - val_accuracy: 0.3378\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 1.3142 - accuracy: 0.5777 - val_loss: 2.0442 - val_accuracy: 0.3467\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 1.1026 - accuracy: 0.6647 - val_loss: 2.5567 - val_accuracy: 0.3156\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 1s 23ms/step - loss: 0.9218 - accuracy: 0.7049 - val_loss: 2.4636 - val_accuracy: 0.2933\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 1s 29ms/step - loss: 0.6652 - accuracy: 0.7799 - val_loss: 2.4046 - val_accuracy: 0.3378\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 1s 27ms/step - loss: 0.5196 - accuracy: 0.8318 - val_loss: 3.0173 - val_accuracy: 0.2800\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.3400 - accuracy: 0.8985 - val_loss: 3.0239 - val_accuracy: 0.3289\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 1s 24ms/step - loss: 0.2501 - accuracy: 0.9203 - val_loss: 4.0968 - val_accuracy: 0.3378\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.2484 - accuracy: 0.9154 - val_loss: 3.7058 - val_accuracy: 0.3378\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 1s 26ms/step - loss: 0.1328 - accuracy: 0.9647 - val_loss: 4.0344 - val_accuracy: 0.2978\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 1s 25ms/step - loss: 0.1310 - accuracy: 0.9610 - val_loss: 4.6591 - val_accuracy: 0.3644\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(min_delta=0.10,patience = 10, monitor='val_loss')\n",
    "\n",
    "history=baseModel.fit(xTrain, yTrain, \n",
    "                      batch_size=32,epochs=100,\n",
    "                      validation_data=(xValidation, yValidation),\n",
    "                      shuffle=True,callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuate the model and predict a classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1s 7ms/step - loss: 4.3799 - accuracy: 0.3501\n"
     ]
    }
   ],
   "source": [
    "baseModel.evaluate(xTest, yTestCategorical)\n",
    "#predict classes\n",
    "prediction = baseModel.predict(xTest)\n",
    "\n",
    "#safe only best class for all prediction \n",
    "yPred = list() \n",
    "for x in prediction: \n",
    "    yPred.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(yTest, yPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remaining cells We perform some plots that can be interested to comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dataSetUtility' has no attribute 'plotConfusionMatrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1924ebda91e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdsu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotConfusionMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"confusion_matrix_1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"images_point_one/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'dataSetUtility' has no attribute 'plotConfusionMatrix'"
     ]
    }
   ],
   "source": [
    "dsu.plotConfusionMatrix(cm, labels, \"confusion_matrix_1\", \"images_point_one/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
