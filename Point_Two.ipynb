{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will to improve our results by building different CNN with different characteristics. We perform data augmentation in order to have more data to train my network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dataSetUtility.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy\n",
    "import cv2 as cv2\n",
    "import numpy as np\n",
    "import keras as ks\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Activation, BatchNormalization, AveragePooling2D, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import import_ipynb\n",
    "import dataSetUtility as dsu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the results we try to increase the number of data following a transformation from left to right. To do this we use a method from the dataSetUtility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class number:  15\n",
      "Class names:  ['Bedroom', 'Coast', 'Forest', 'Highway', 'Industrial', 'InsideCity', 'Kitchen', 'LivingRoom', 'Mountain', 'Office', 'OpenCountry', 'Store', 'Street', 'Suburb', 'TallBuilding']\n"
     ]
    }
   ],
   "source": [
    "#path to the directories\n",
    "pathTrain=r'C:\\Users\\adria\\Desktop\\CVPR\\ImageSet\\train'\n",
    "pathTest=r'C:\\Users\\adria\\Desktop\\CVPR\\ImageSet\\test'\n",
    "\n",
    "labels = [os.path.basename(i) for i in glob.glob(pathTrain + '/*', recursive=True)]\n",
    "numberOfClasses = len(labels)\n",
    "print(\"Class number: \", numberOfClasses)\n",
    "print(\"Class names: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len:  3000\n",
      "Train label len:  3000\n",
      "---------------------------------\n",
      "Test len:  2985\n",
      "Test label len:  2985\n"
     ]
    }
   ],
   "source": [
    "xTrainAug, yTrainAug = dsu.dataAugmentation(pathTrain, labels)\n",
    "xTest, yTest = dsu.loadImages(pathTest, labels)\n",
    "print(\"Train len: \", len(xTrainAug))\n",
    "print(\"Train label len: \", len(yTrainAug))\n",
    "print(\"---------------------------------\")\n",
    "print(\"Test len: \", len(xTest))\n",
    "print(\"Test label len: \", len(yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainCategorical = to_categorical(yTrainAug)\n",
    "yTestCategorical = to_categorical(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len Train Set:  2550\n",
      "len Validation Set:  450\n",
      "len Test Set:  2985\n"
     ]
    }
   ],
   "source": [
    "xTrain, xValidation, yTrain, yValidation = train_test_split(xTrainAug, yTrainCategorical, train_size=0.85, random_state=275)\n",
    "print('len Train Set: ',len(xTrain))\n",
    "print('len Validation Set: ',len(xValidation))\n",
    "print('len Test Set: ',len(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we try to apply the data augmentation to the cnn of point one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = ks.optimizers.SGD(momentum=0.9,nesterov=True)\n",
    "norm = ks.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "baseModel = Sequential([\n",
    "    #first convolutional layer\n",
    "    Conv2D(8, 3,strides=1, padding='valid', activation='relu', input_shape=(64,64,1)),\n",
    "    MaxPooling2D(pool_size=2,strides=2),\n",
    "    \n",
    "    #second convolutional layer\n",
    "    Conv2D(16, 3,strides=1, padding='valid',activation='relu',input_shape=(64,64,1)),\n",
    "    MaxPooling2D(pool_size=2,strides=2),\n",
    "    \n",
    "    #third convolutional layer\n",
    "    Conv2D(32, 3,strides=1, padding='valid',activation='relu',input_shape=(64,64,1)),\n",
    "    Flatten(),\n",
    "    Dense(numberOfClasses, activation='relu',kernel_initializer=norm, bias_initializer='zeros'),\n",
    "    Dense(numberOfClasses, activation='softmax')\n",
    "])\n",
    "\n",
    "baseModel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 3s 31ms/step - loss: 2.7089 - accuracy: 0.0546 - val_loss: 2.7017 - val_accuracy: 0.0644\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 2.6891 - accuracy: 0.0781 - val_loss: 2.6718 - val_accuracy: 0.1089\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 2.6144 - accuracy: 0.1411 - val_loss: 2.6545 - val_accuracy: 0.1578\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 1s 19ms/step - loss: 2.5278 - accuracy: 0.1682 - val_loss: 2.4485 - val_accuracy: 0.2089\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 2.3059 - accuracy: 0.2488 - val_loss: 2.2391 - val_accuracy: 0.2467\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 2.0689 - accuracy: 0.3102 - val_loss: 2.1331 - val_accuracy: 0.2956\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 1.9540 - accuracy: 0.3590 - val_loss: 2.0232 - val_accuracy: 0.3489\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 1.7496 - accuracy: 0.3943 - val_loss: 1.9705 - val_accuracy: 0.3756\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 1.5927 - accuracy: 0.4802 - val_loss: 1.7473 - val_accuracy: 0.4200\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 1.3177 - accuracy: 0.5730 - val_loss: 1.7367 - val_accuracy: 0.4200\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 1.1170 - accuracy: 0.6154 - val_loss: 1.9769 - val_accuracy: 0.3911\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.9764 - accuracy: 0.6716 - val_loss: 1.8352 - val_accuracy: 0.4267\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.7200 - accuracy: 0.7592 - val_loss: 2.0135 - val_accuracy: 0.4044\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.5960 - accuracy: 0.8011 - val_loss: 2.4330 - val_accuracy: 0.4200\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 1s 19ms/step - loss: 0.4569 - accuracy: 0.8518 - val_loss: 2.7070 - val_accuracy: 0.4378\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.3442 - accuracy: 0.8880 - val_loss: 2.7191 - val_accuracy: 0.4267\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.2513 - accuracy: 0.9170 - val_loss: 3.0726 - val_accuracy: 0.4244\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.2602 - accuracy: 0.9138 - val_loss: 3.4152 - val_accuracy: 0.4178\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.2169 - accuracy: 0.9244 - val_loss: 3.5819 - val_accuracy: 0.4511\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(min_delta=0.10,patience = 10, monitor='val_loss')\n",
    "\n",
    "history=baseModel.fit(xTrain, yTrain, \n",
    "                      batch_size=32,epochs=100,\n",
    "                      validation_data=(xValidation, yValidation),\n",
    "                      shuffle=True,callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
